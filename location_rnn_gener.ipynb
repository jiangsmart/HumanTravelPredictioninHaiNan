{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入训练路径\n",
    "train_path = '/home/jiang/data/sorted10000'\n",
    "# 基站路径\n",
    "station_path = '/home/jiang/data/stationRecord'\n",
    "# 输入长度\n",
    "maxlen = 150\n",
    "# 输入中某地（如海口）大于该值，进入训练集\n",
    "threshold = 120\n",
    "# 滚动步长\n",
    "step = 1\n",
    "# 测试长度\n",
    "testlen = 50\n",
    "batch_size=16\n",
    "num_skips=2\n",
    "skip_window=2\n",
    "data_index1=0\n",
    "data_index2=0\n",
    "station_size=200\n",
    "lr=0.01\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "num_steps = 2000\n",
    "\n",
    "station_size = 10000\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(train_path):\n",
    "    file_object = open(train_path)\n",
    "    train_list = file_object.readlines()\n",
    "    train_set=[]\n",
    "    user_record=[]\n",
    "    station_list=[]\n",
    "    station_set=set()\n",
    "    for i,record in enumerate(train_list):\n",
    "        record=record.strip()\n",
    "        user,station=(record.split(\",\")[j] for j in [0,3])\n",
    "        station_set.add(int(station))\n",
    "        if i == len(train_list)-1:\n",
    "            user_record.append(int(station))\n",
    "            train_set.append(user_record)\n",
    "            station_list=sorted(list(station_set))\n",
    "            return train_set,station_list\n",
    "        elif user != train_list[i+1].split(\",\")[0]:\n",
    "            user_record.append(int(station))\n",
    "            train_set.append(user_record)\n",
    "            user_record=[]\n",
    "        else:\n",
    "            user_record.append(int(station))\n",
    "    #return train_set,station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意不要让序列小于3个的序列进入\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index1\n",
    "    global data_index2\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    #print len(data[data_index1])\n",
    "    for i in range(batch_size // num_skips):\n",
    "        #第一次进入循环初始化buffer\n",
    "        if i == 0:\n",
    "            while len(data[data_index1])-data_index2 < span:\n",
    "                data_index2 = 0\n",
    "                data_index1 = (data_index1 + 1)%len(data)                \n",
    "            for _ in range(span-1):\n",
    "                buffer.append(data[data_index1][data_index2])\n",
    "                data_index2 += 1\n",
    "        ###############################\n",
    "        #index更新后判断是否越界\n",
    "        if data_index2 == (len(data[data_index1])):\n",
    "            data_index2 = 0\n",
    "            data_index1 = (data_index1 + 1)%len(data)\n",
    "            while len(data[data_index1])-data_index2 < span:\n",
    "                data_index1 = (data_index1 + 1)%len(data)\n",
    "            for _ in range(span-1):\n",
    "                buffer.append(data[data_index1][data_index2])\n",
    "                data_index2 += 1\n",
    "        #################################\n",
    "        #print data_index1,data_index2 \n",
    "        buffer.append(data[data_index1][data_index2])\n",
    "        #print buffer\n",
    "        data_index2 += 1\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    #data_index2 = (data_index2 + len(data) - span) % len(data)\n",
    "    return batch, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recount_data(data,station_indices):\n",
    "    new_data=[]\n",
    "    for record in data:\n",
    "        new_record=[]\n",
    "        for station in record:\n",
    "            new_record.append(station_indices[station])\n",
    "        new_data.append(new_record)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_pre(train_path):\n",
    "    train_data,station_data=read_data(train_path)\n",
    "    sum_station = len(station_data)\n",
    "    station_indices = dict((c, i) for i, c in enumerate(station_data))\n",
    "    indices_station = dict((i, c) for i, c in enumerate(station_data))\n",
    "    train_data = recount_data(train_data,station_indices)\n",
    "    return train_data,sum_station,indices_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183\n"
     ]
    }
   ],
   "source": [
    "data,station_num,indices_station=data_pre(train_path)\n",
    "print station_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "      tf.random_uniform([station_num, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    \n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([station_num, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([station_num]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=station_num))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(nce_loss)\n",
    "    \n",
    "#   # Construct the variables for the softmax\n",
    "#     weights = tf.Variable(tf.truncated_normal([embedding_size, station_num],\n",
    "#                                               stddev=1.0 / math.sqrt(embedding_size)))\n",
    "#     biases = tf.Variable(tf.zeros([station_num]))\n",
    "#     hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "\n",
    "#   # convert train_context to a one-hot format\n",
    "#     train_one_hot = tf.one_hot(train_context, station_num)\n",
    "\n",
    "#     cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "\n",
    "#   # Construct the SGD optimizer using a learning rate of lr.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to 10673: 10802, 43473, 11583, 43842, 42591, 30363, 13742, 12391,\n",
      "Nearest to 10491: 38154, 11672, 19282, 11522, 12402, 43513, 11383, 31352,\n",
      "Nearest to 10522: 13791, 13221, 18251, 19261, 15313, 14732, 11763, 13524,\n",
      "Nearest to 10062: 18061, 14452, 12553, 23451, 38154, 24211, 10351, 14754,\n",
      "Nearest to 10773: 11211, 14811, 12083, 38159, 11722, 44043, 42501, 15662,\n",
      "Nearest to 10242: 12263, 12531, 11053, 14751, 19172, 13951, 22036, 10643,\n",
      "Nearest to 10106: 21421, 42321, 43512, 13713, 43053, 16235, 17022, 14732,\n",
      "Nearest to 10333: 42421, 42461, 11811, 24262, 13843, 40303, 43221, 11522,\n",
      "Nearest to 10202: 10011, 42521, 40303, 15313, 19933, 13762, 47571, 39156,\n",
      "Nearest to 10661: 17012, 12023, 13303, 13093, 10683, 31932, 31691, 13571,\n",
      "Nearest to 10221: 11073, 44231, 11783, 11303, 13561, 47541, 12651, 21482,\n",
      "Nearest to 10642: 24386, 48311, 11001, 21492, 30922, 12481, 12002, 11003,\n",
      "Nearest to 10331: 42773, 13243, 13562, 42221, 10223, 11781, 44041, 18564,\n",
      "Nearest to 10782: 13571, 48311, 13851, 42981, 42051, 19941, 44611, 42592,\n",
      "Nearest to 10544: 13773, 10401, 60362, 36833, 17272, 13441, 10201, 11812,\n",
      "Nearest to 10531: 60243, 11002, 19093, 13023, 42193, 22452, 13122, 21923,\n",
      "('Average loss at step ', 10, ': ', 165.42801513671876)\n",
      "('Average loss at step ', 20, ': ', 152.91956634521483)\n",
      "('Average loss at step ', 30, ': ', 151.94602813720704)\n",
      "('Average loss at step ', 40, ': ', 153.97229614257813)\n",
      "('Average loss at step ', 50, ': ', 148.12550659179686)\n",
      "('Average loss at step ', 60, ': ', 153.482421875)\n",
      "('Average loss at step ', 70, ': ', 148.93083801269532)\n",
      "('Average loss at step ', 80, ': ', 147.37233581542969)\n",
      "('Average loss at step ', 90, ': ', 150.93032531738282)\n",
      "('Average loss at step ', 100, ': ', 146.92743148803712)\n",
      "('Average loss at step ', 110, ': ', 147.73937377929687)\n",
      "('Average loss at step ', 120, ': ', 150.67648315429688)\n",
      "('Average loss at step ', 130, ': ', 154.49928283691406)\n",
      "('Average loss at step ', 140, ': ', 151.04090423583983)\n",
      "('Average loss at step ', 150, ': ', 151.12243194580077)\n",
      "('Average loss at step ', 160, ': ', 151.44375152587889)\n",
      "('Average loss at step ', 170, ': ', 151.96233520507812)\n",
      "('Average loss at step ', 180, ': ', 146.33038482666015)\n",
      "('Average loss at step ', 190, ': ', 148.46619720458983)\n",
      "('Average loss at step ', 200, ': ', 149.25539855957032)\n",
      "Nearest to 10673: 43171, 15332, 42584, 22352, 44805, 42641, 43741, 43013,\n",
      "Nearest to 10491: 12401, 10513, 19031, 14772, 42941, 13242, 13471, 12651,\n",
      "Nearest to 10522: 13134, 19213, 11001, 32963, 43281, 11793, 12463, 11201,\n",
      "Nearest to 10062: 13943, 11583, 38127, 21081, 11831, 11383, 12091, 44041,\n",
      "Nearest to 10773: 14013, 13842, 42491, 14581, 24332, 40263, 15211, 21043,\n",
      "Nearest to 10242: 19091, 47183, 43511, 48231, 43733, 41383, 13134, 12892,\n",
      "Nearest to 10106: 14071, 43021, 11781, 43001, 43292, 30513, 11811, 44032,\n",
      "Nearest to 10333: 31353, 10953, 30153, 42441, 22661, 22101, 24383, 43495,\n",
      "Nearest to 10202: 43671, 17323, 40472, 14383, 25793, 11672, 50523, 11763,\n",
      "Nearest to 10661: 19021, 11303, 13802, 32396, 43461, 14811, 22741, 24262,\n",
      "Nearest to 10221: 21592, 42311, 11752, 38154, 48311, 11531, 42511, 30663,\n",
      "Nearest to 10642: 42523, 19711, 43432, 47302, 43762, 40303, 13532, 13023,\n",
      "Nearest to 10331: 13713, 22661, 11922, 12973, 50243, 11831, 12481, 47053,\n",
      "Nearest to 10782: 42302, 21123, 31352, 23263, 32396, 13372, 17391, 18503,\n",
      "Nearest to 10544: 43461, 47552, 10973, 12062, 13913, 24211, 30251, 40303,\n",
      "Nearest to 10531: 43013, 22351, 21923, 50243, 40481, 10913, 22781, 11672,\n",
      "('Average loss at step ', 210, ': ', 148.59969940185547)\n",
      "('Average loss at step ', 220, ': ', 153.72017364501954)\n",
      "('Average loss at step ', 230, ': ', 148.34241485595703)\n",
      "('Average loss at step ', 240, ': ', 148.48293609619139)\n",
      "('Average loss at step ', 250, ': ', 147.79698333740234)\n",
      "('Average loss at step ', 260, ': ', 149.66515502929687)\n",
      "('Average loss at step ', 270, ': ', 144.54777984619142)\n",
      "('Average loss at step ', 280, ': ', 151.987060546875)\n",
      "('Average loss at step ', 290, ': ', 149.21219940185546)\n",
      "('Average loss at step ', 300, ': ', 151.26049194335937)\n",
      "('Average loss at step ', 310, ': ', 147.67835845947266)\n",
      "('Average loss at step ', 320, ': ', 150.60357513427735)\n",
      "('Average loss at step ', 330, ': ', 153.20734558105468)\n",
      "('Average loss at step ', 340, ': ', 150.25461883544921)\n",
      "('Average loss at step ', 350, ': ', 145.63019561767578)\n",
      "('Average loss at step ', 360, ': ', 149.57190551757813)\n",
      "('Average loss at step ', 370, ': ', 145.09031524658204)\n",
      "('Average loss at step ', 380, ': ', 153.90281066894531)\n",
      "('Average loss at step ', 390, ': ', 153.82608795166016)\n",
      "('Average loss at step ', 400, ': ', 149.96678009033204)\n",
      "Nearest to 10673: 18881, 12513, 42661, 50462, 11133, 47181, 15672, 40263,\n",
      "Nearest to 10491: 14011, 44044, 12393, 11142, 42911, 47051, 15402, 13233,\n",
      "Nearest to 10522: 21043, 11481, 19812, 43021, 10653, 44102, 25551, 38129,\n",
      "Nearest to 10062: 15641, 13512, 10413, 42411, 44011, 50173, 14042, 43512,\n",
      "Nearest to 10773: 14112, 50061, 12553, 17022, 13033, 42244, 10311, 12483,\n",
      "Nearest to 10242: 13213, 40833, 12503, 13093, 50151, 14754, 13031, 13013,\n",
      "Nearest to 10106: 12563, 17391, 13742, 10642, 43671, 13081, 44302, 12232,\n",
      "Nearest to 10333: 10762, 13373, 12142, 42312, 10301, 43983, 25793, 42193,\n",
      "Nearest to 10202: 25251, 12012, 11732, 12531, 10463, 44043, 12611, 44032,\n",
      "Nearest to 10661: 11922, 11023, 21421, 10491, 41262, 47212, 21123, 48311,\n",
      "Nearest to 10221: 12081, 11133, 13841, 43953, 10152, 43731, 13113, 42661,\n",
      "Nearest to 10642: 10953, 42272, 12093, 43122, 43341, 10106, 24262, 43281,\n",
      "Nearest to 10331: 22561, 17022, 10072, 21582, 13502, 41625, 43312, 44206,\n",
      "Nearest to 10782: 43623, 42921, 50562, 17351, 60202, 60243, 19275, 31933,\n",
      "Nearest to 10544: 12896, 14754, 33004, 11801, 18852, 12891, 13501, 13061,\n",
      "Nearest to 10531: 10671, 18252, 12922, 13353, 42031, 11731, 44204, 18853,\n",
      "('Average loss at step ', 410, ': ', 146.35615692138671)\n",
      "('Average loss at step ', 420, ': ', 143.43516845703124)\n",
      "('Average loss at step ', 430, ': ', 149.90193328857421)\n",
      "('Average loss at step ', 440, ': ', 147.98215637207031)\n",
      "('Average loss at step ', 450, ': ', 145.53868103027344)\n",
      "('Average loss at step ', 460, ': ', 152.98208618164062)\n",
      "('Average loss at step ', 470, ': ', 149.69797058105468)\n",
      "('Average loss at step ', 480, ': ', 144.25174560546876)\n",
      "('Average loss at step ', 490, ': ', 152.20676422119141)\n",
      "('Average loss at step ', 500, ': ', 150.19774322509767)\n",
      "('Average loss at step ', 510, ': ', 149.27759704589843)\n",
      "('Average loss at step ', 520, ': ', 146.92141723632812)\n",
      "('Average loss at step ', 530, ': ', 150.43306884765624)\n",
      "('Average loss at step ', 540, ': ', 151.50929260253906)\n",
      "('Average loss at step ', 550, ': ', 151.21854095458986)\n",
      "('Average loss at step ', 560, ': ', 148.65869903564453)\n",
      "('Average loss at step ', 570, ': ', 144.24913635253907)\n",
      "('Average loss at step ', 580, ': ', 145.58524780273439)\n",
      "('Average loss at step ', 590, ': ', 151.35466918945312)\n",
      "('Average loss at step ', 600, ': ', 148.17400817871095)\n",
      "Nearest to 10673: 21012, 42652, 43831, 43496, 42561, 38129, 50595, 12503,\n",
      "Nearest to 10491: 43961, 21011, 40481, 12823, 18251, 43042, 11801, 60473,\n",
      "Nearest to 10522: 11811, 17323, 24056, 10383, 15681, 42011, 11351, 12393,\n",
      "Nearest to 10062: 42221, 17042, 30073, 24503, 14311, 10653, 47342, 43522,\n",
      "Nearest to 10773: 19283, 13851, 24333, 14383, 10522, 47583, 60261, 11941,\n",
      "Nearest to 10242: 11033, 12662, 15093, 47581, 22781, 10912, 13511, 24332,\n",
      "Nearest to 10106: 12264, 13061, 32262, 11723, 24383, 32062, 47302, 42662,\n",
      "Nearest to 10333: 48311, 42511, 42022, 60055, 13742, 10491, 11762, 47053,\n",
      "Nearest to 10202: 12541, 14462, 17501, 47586, 13802, 11391, 13371, 43865,\n",
      "Nearest to 10661: 14752, 11133, 23102, 11831, 11463, 43842, 11583, 43432,\n",
      "Nearest to 10221: 13321, 11703, 11303, 11832, 42203, 11821, 13123, 22351,\n",
      "Nearest to 10642: 18061, 11922, 15614, 42121, 44804, 17392, 44041, 19115,\n",
      "Nearest to 10331: 42602, 43425, 13352, 32963, 10841, 19011, 42351, 12131,\n",
      "Nearest to 10782: 10911, 14072, 42512, 11821, 42202, 11872, 22052, 13132,\n",
      "Nearest to 10544: 19711, 14732, 18853, 10921, 11732, 44602, 11701, 42971,\n",
      "Nearest to 10531: 18061, 10771, 42471, 30251, 10473, 10331, 44041, 12721,\n",
      "('Average loss at step ', 610, ': ', 151.74109344482423)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step ', 620, ': ', 149.24222564697266)\n",
      "('Average loss at step ', 630, ': ', 151.1774932861328)\n",
      "('Average loss at step ', 640, ': ', 150.27618408203125)\n",
      "('Average loss at step ', 650, ': ', 151.46022796630859)\n",
      "('Average loss at step ', 660, ': ', 145.04606323242189)\n",
      "('Average loss at step ', 670, ': ', 150.24979858398439)\n",
      "('Average loss at step ', 680, ': ', 147.97459411621094)\n",
      "('Average loss at step ', 690, ': ', 147.29170074462891)\n",
      "('Average loss at step ', 700, ': ', 146.66934967041016)\n",
      "('Average loss at step ', 710, ': ', 145.6235321044922)\n",
      "('Average loss at step ', 720, ': ', 150.06959381103516)\n",
      "('Average loss at step ', 730, ': ', 145.04517974853516)\n",
      "('Average loss at step ', 740, ': ', 149.83752899169923)\n",
      "('Average loss at step ', 750, ': ', 150.62796325683593)\n",
      "('Average loss at step ', 760, ': ', 151.07276000976563)\n",
      "('Average loss at step ', 770, ': ', 150.31548309326172)\n",
      "('Average loss at step ', 780, ': ', 150.98615722656251)\n",
      "('Average loss at step ', 790, ': ', 148.25940551757813)\n",
      "('Average loss at step ', 800, ': ', 146.99069824218751)\n",
      "Nearest to 10673: 14771, 43522, 19182, 12533, 43491, 14772, 14903, 43221,\n",
      "Nearest to 10491: 13441, 11721, 39183, 11383, 43733, 11081, 11762, 43041,\n",
      "Nearest to 10522: 15391, 43413, 13321, 20062, 42231, 12264, 43011, 50121,\n",
      "Nearest to 10062: 10141, 41624, 14041, 14813, 43672, 18252, 50561, 21451,\n",
      "Nearest to 10773: 13523, 14301, 47583, 11811, 49171, 44044, 43432, 10383,\n",
      "Nearest to 10242: 13303, 31811, 11835, 12481, 40472, 42191, 19115, 13022,\n",
      "Nearest to 10106: 14734, 18045, 15403, 10544, 21993, 13661, 13013, 11962,\n",
      "Nearest to 10333: 47051, 24262, 10822, 13135, 13582, 19841, 22106, 12121,\n",
      "Nearest to 10202: 42811, 42193, 13501, 43761, 10793, 43741, 22102, 19183,\n",
      "Nearest to 10661: 12391, 43343, 22193, 10081, 43511, 11811, 14755, 18071,\n",
      "Nearest to 10221: 10351, 10401, 18256, 13711, 12923, 21042, 19801, 14393,\n",
      "Nearest to 10642: 21555, 30391, 12121, 14756, 12491, 44205, 47803, 15061,\n",
      "Nearest to 10331: 12113, 10783, 10251, 30663, 42383, 15402, 43961, 11732,\n",
      "Nearest to 10782: 14311, 49203, 42813, 10143, 18251, 15431, 14761, 42221,\n",
      "Nearest to 10544: 15093, 12893, 13341, 14031, 38127, 10106, 12461, 17351,\n",
      "Nearest to 10531: 19401, 12923, 43163, 10523, 11133, 14752, 14312, 50281,\n",
      "('Average loss at step ', 810, ': ', 144.49101257324219)\n",
      "('Average loss at step ', 820, ': ', 152.57114868164064)\n",
      "('Average loss at step ', 830, ': ', 148.41338043212892)\n",
      "('Average loss at step ', 840, ': ', 150.16551971435547)\n",
      "('Average loss at step ', 850, ': ', 152.26005401611329)\n",
      "('Average loss at step ', 860, ': ', 146.87925720214844)\n",
      "('Average loss at step ', 870, ': ', 147.66864624023438)\n",
      "('Average loss at step ', 880, ': ', 150.25135650634766)\n",
      "('Average loss at step ', 890, ': ', 146.64265823364258)\n",
      "('Average loss at step ', 900, ': ', 149.49382629394532)\n",
      "('Average loss at step ', 910, ': ', 146.23598327636719)\n",
      "('Average loss at step ', 920, ': ', 146.67680664062499)\n",
      "('Average loss at step ', 930, ': ', 147.18488311767578)\n",
      "('Average loss at step ', 940, ': ', 147.77694091796874)\n",
      "('Average loss at step ', 950, ': ', 154.03816528320311)\n",
      "('Average loss at step ', 960, ': ', 148.27157897949218)\n",
      "('Average loss at step ', 970, ': ', 148.77192840576171)\n",
      "('Average loss at step ', 980, ': ', 150.14102630615236)\n",
      "('Average loss at step ', 990, ': ', 146.50757904052733)\n",
      "('Average loss at step ', 1000, ': ', 155.06557159423829)\n",
      "Nearest to 10673: 44102, 43512, 14192, 10472, 42523, 22111, 43513, 12091,\n",
      "Nearest to 10491: 22106, 42982, 43281, 44205, 31933, 49171, 11962, 19213,\n",
      "Nearest to 10522: 19261, 11835, 42911, 15673, 11392, 10663, 17132, 12483,\n",
      "Nearest to 10062: 43281, 11761, 24581, 44282, 42971, 13512, 13842, 22193,\n",
      "Nearest to 10773: 14223, 15091, 13342, 47053, 14801, 43362, 44091, 42652,\n",
      "Nearest to 10242: 14073, 40303, 42701, 10413, 17022, 11083, 13841, 17023,\n",
      "Nearest to 10106: 13102, 42321, 43431, 15673, 42383, 17351, 49222, 19512,\n",
      "Nearest to 10333: 44113, 19091, 12553, 47342, 13353, 22453, 50121, 14761,\n",
      "Nearest to 10202: 43973, 18182, 13272, 39156, 41624, 24383, 14225, 11071,\n",
      "Nearest to 10661: 13583, 43603, 24262, 42592, 10793, 44022, 30723, 43082,\n",
      "Nearest to 10221: 43011, 10653, 13841, 13242, 11391, 17133, 22311, 13321,\n",
      "Nearest to 10642: 11213, 30363, 11532, 12001, 10953, 24287, 40472, 12515,\n",
      "Nearest to 10331: 12263, 42623, 22052, 14581, 10951, 60383, 13471, 13791,\n",
      "Nearest to 10782: 13663, 13542, 18853, 12641, 43241, 10802, 42411, 43741,\n",
      "Nearest to 10544: 42981, 22351, 15402, 44802, 14581, 10021, 10301, 60373,\n",
      "Nearest to 10531: 47342, 43966, 44032, 18853, 12821, 21953, 14753, 12113,\n",
      "('Average loss at step ', 1010, ': ', 149.46713867187501)\n",
      "('Average loss at step ', 1020, ': ', 150.04608001708985)\n",
      "('Average loss at step ', 1030, ': ', 144.57034454345703)\n",
      "('Average loss at step ', 1040, ': ', 150.04539184570314)\n",
      "('Average loss at step ', 1050, ': ', 149.22515411376952)\n",
      "('Average loss at step ', 1060, ': ', 150.63261871337892)\n",
      "('Average loss at step ', 1070, ': ', 147.09917297363282)\n",
      "('Average loss at step ', 1080, ': ', 150.9124771118164)\n",
      "('Average loss at step ', 1090, ': ', 151.87516632080079)\n",
      "('Average loss at step ', 1100, ': ', 146.72261352539061)\n",
      "('Average loss at step ', 1110, ': ', 152.84041442871094)\n",
      "('Average loss at step ', 1120, ': ', 145.08013458251952)\n",
      "('Average loss at step ', 1130, ': ', 145.97859497070311)\n",
      "('Average loss at step ', 1140, ': ', 150.8619415283203)\n",
      "('Average loss at step ', 1150, ': ', 151.01655883789061)\n",
      "('Average loss at step ', 1160, ': ', 152.22387237548827)\n",
      "('Average loss at step ', 1170, ': ', 151.72995300292968)\n",
      "('Average loss at step ', 1180, ': ', 148.53316497802734)\n",
      "('Average loss at step ', 1190, ': ', 151.31740570068359)\n",
      "('Average loss at step ', 1200, ': ', 148.6313461303711)\n",
      "Nearest to 10673: 19022, 47211, 12113, 11463, 32112, 14072, 10636, 42772,\n",
      "Nearest to 10491: 11053, 43965, 43242, 19272, 43253, 32111, 14581, 19932,\n",
      "Nearest to 10522: 22453, 11702, 43965, 10401, 42793, 42401, 14762, 19811,\n",
      "Nearest to 10062: 47181, 11702, 43051, 12651, 43292, 10842, 12643, 15681,\n",
      "Nearest to 10773: 42231, 14762, 24052, 49222, 50511, 14212, 50093, 11211,\n",
      "Nearest to 10242: 42793, 11733, 10661, 22101, 14043, 19442, 10953, 47202,\n",
      "Nearest to 10106: 10352, 42865, 50281, 15403, 19114, 11351, 39156, 43041,\n",
      "Nearest to 10333: 22531, 36833, 22561, 13093, 13952, 42801, 21483, 14732,\n",
      "Nearest to 10202: 11043, 19943, 10653, 21042, 42961, 10652, 42441, 43861,\n",
      "Nearest to 10661: 19022, 43711, 21081, 10242, 31461, 12121, 49222, 13011,\n",
      "Nearest to 10221: 38127, 21583, 42591, 17173, 21123, 12393, 42452, 13122,\n",
      "Nearest to 10642: 13211, 43011, 42263, 12393, 22111, 60202, 11073, 13841,\n",
      "Nearest to 10331: 43221, 10912, 49201, 12921, 31691, 24261, 12482, 50451,\n",
      "Nearest to 10782: 14273, 42032, 19011, 42813, 47213, 13943, 22101, 42503,\n",
      "Nearest to 10544: 42351, 44811, 38129, 11463, 19712, 41591, 38159, 11532,\n",
      "Nearest to 10531: 17391, 10523, 11231, 44231, 40021, 22453, 12892, 40392,\n",
      "('Average loss at step ', 1210, ': ', 152.60786132812501)\n",
      "('Average loss at step ', 1220, ': ', 150.02593765258788)\n",
      "('Average loss at step ', 1230, ': ', 148.82023315429689)\n",
      "('Average loss at step ', 1240, ': ', 145.31907043457031)\n",
      "('Average loss at step ', 1250, ': ', 152.75740203857421)\n",
      "('Average loss at step ', 1260, ': ', 148.18577575683594)\n",
      "('Average loss at step ', 1270, ': ', 149.55754852294922)\n",
      "('Average loss at step ', 1280, ': ', 148.12264709472657)\n",
      "('Average loss at step ', 1290, ': ', 143.56917877197264)\n",
      "('Average loss at step ', 1300, ': ', 149.40870971679686)\n",
      "('Average loss at step ', 1310, ': ', 148.88145294189454)\n",
      "('Average loss at step ', 1320, ': ', 149.34152069091797)\n",
      "('Average loss at step ', 1330, ': ', 152.22574768066406)\n",
      "('Average loss at step ', 1340, ': ', 148.14973907470704)\n",
      "('Average loss at step ', 1350, ': ', 146.56118164062499)\n",
      "('Average loss at step ', 1360, ': ', 156.39589843749999)\n",
      "('Average loss at step ', 1370, ': ', 149.67277832031249)\n",
      "('Average loss at step ', 1380, ': ', 154.57359313964844)\n",
      "('Average loss at step ', 1390, ': ', 149.94956970214844)\n",
      "('Average loss at step ', 1400, ': ', 146.40737152099609)\n",
      "Nearest to 10673: 13821, 50523, 11213, 42022, 30251, 18101, 13122, 42493,\n",
      "Nearest to 10491: 42192, 21041, 13523, 42252, 14451, 42624, 13531, 12822,\n",
      "Nearest to 10522: 13712, 50451, 44085, 10201, 23072, 43343, 12433, 32462,\n",
      "Nearest to 10062: 15662, 12561, 44103, 15651, 17323, 43263, 14311, 10842,\n",
      "Nearest to 10773: 40011, 50462, 12061, 42812, 19493, 11303, 12113, 19153,\n",
      "Nearest to 10242: 14072, 18564, 32262, 44233, 11831, 14381, 15614, 14551,\n",
      "Nearest to 10106: 31932, 11481, 10232, 36833, 13103, 44172, 17042, 19161,\n",
      "Nearest to 10333: 44451, 41442, 13223, 40834, 11613, 20251, 11752, 42561,\n",
      "Nearest to 10202: 36833, 43171, 42204, 11732, 10432, 41262, 19032, 23102,\n",
      "Nearest to 10661: 10331, 10883, 12894, 43762, 42244, 42811, 31461, 13231,\n",
      "Nearest to 10221: 14301, 43002, 30912, 50161, 13591, 39183, 12393, 10812,\n",
      "Nearest to 10642: 48231, 42631, 15091, 19932, 24263, 47065, 14811, 13272,\n",
      "Nearest to 10331: 42151, 43983, 10661, 42412, 50151, 14531, 12242, 13751,\n",
      "Nearest to 10782: 12091, 42653, 42773, 21425, 43831, 42192, 12891, 11021,\n",
      "Nearest to 10544: 40472, 43425, 24574, 44611, 42571, 14631, 42272, 11801,\n",
      "Nearest to 10531: 42362, 18101, 43373, 15313, 25551, 39113, 13881, 47571,\n",
      "('Average loss at step ', 1410, ': ', 153.01837463378905)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step ', 1420, ': ', 155.26338500976561)\n",
      "('Average loss at step ', 1430, ': ', 147.68453521728514)\n",
      "('Average loss at step ', 1440, ': ', 145.86368255615236)\n",
      "('Average loss at step ', 1450, ': ', 154.43888549804689)\n",
      "('Average loss at step ', 1460, ': ', 147.3147003173828)\n",
      "('Average loss at step ', 1470, ': ', 147.86567840576171)\n",
      "('Average loss at step ', 1480, ': ', 145.98232269287109)\n",
      "('Average loss at step ', 1490, ': ', 146.05819854736328)\n",
      "('Average loss at step ', 1500, ': ', 155.90278472900391)\n",
      "('Average loss at step ', 1510, ': ', 144.52509613037108)\n",
      "('Average loss at step ', 1520, ': ', 151.37565307617189)\n",
      "('Average loss at step ', 1530, ': ', 148.39995422363282)\n",
      "('Average loss at step ', 1540, ': ', 149.00134887695313)\n",
      "('Average loss at step ', 1550, ': ', 149.27793884277344)\n",
      "('Average loss at step ', 1560, ': ', 150.00018157958985)\n",
      "('Average loss at step ', 1570, ': ', 149.68550415039061)\n",
      "('Average loss at step ', 1580, ': ', 148.85749969482421)\n",
      "('Average loss at step ', 1590, ': ', 151.59922943115234)\n",
      "('Average loss at step ', 1600, ': ', 145.05623168945311)\n",
      "Nearest to 10673: 43983, 22803, 14122, 43603, 50111, 13711, 50442, 11153,\n",
      "Nearest to 10491: 17392, 14734, 47342, 14762, 14821, 12231, 10752, 19941,\n",
      "Nearest to 10522: 11811, 38129, 39183, 13033, 11061, 15652, 10841, 10191,\n",
      "Nearest to 10062: 42812, 47141, 47312, 31691, 13762, 17121, 30422, 22111,\n",
      "Nearest to 10773: 12561, 40021, 60293, 17351, 17263, 43242, 14733, 42302,\n",
      "Nearest to 10242: 17173, 13822, 19172, 11522, 19572, 11722, 15211, 13062,\n",
      "Nearest to 10106: 42081, 20322, 30671, 43833, 22412, 42312, 18102, 24263,\n",
      "Nearest to 10333: 47262, 11391, 14203, 12501, 12461, 11963, 44042, 14452,\n",
      "Nearest to 10202: 43362, 17263, 10544, 43731, 15661, 42092, 33004, 17613,\n",
      "Nearest to 10661: 11916, 21923, 22282, 13095, 11053, 42072, 12263, 19182,\n",
      "Nearest to 10221: 41383, 10413, 17501, 47182, 13582, 31071, 15062, 24262,\n",
      "Nearest to 10642: 21012, 19071, 10953, 14122, 11672, 39113, 18031, 12793,\n",
      "Nearest to 10331: 30521, 47051, 21451, 13211, 12663, 13933, 24052, 12873,\n",
      "Nearest to 10782: 19811, 14012, 13362, 19932, 42273, 42022, 14041, 44113,\n",
      "Nearest to 10544: 12892, 21041, 41351, 18031, 11331, 10841, 22101, 42111,\n",
      "Nearest to 10531: 10636, 13561, 11383, 47211, 13523, 41442, 11781, 22052,\n",
      "('Average loss at step ', 1610, ': ', 147.83363342285156)\n",
      "('Average loss at step ', 1620, ': ', 149.4324951171875)\n",
      "('Average loss at step ', 1630, ': ', 150.43454437255861)\n",
      "('Average loss at step ', 1640, ': ', 150.74620666503907)\n",
      "('Average loss at step ', 1650, ': ', 151.27362060546875)\n",
      "('Average loss at step ', 1660, ': ', 152.53875274658202)\n",
      "('Average loss at step ', 1670, ': ', 153.31307830810547)\n",
      "('Average loss at step ', 1680, ': ', 150.19188995361327)\n",
      "('Average loss at step ', 1690, ': ', 146.92917175292968)\n",
      "('Average loss at step ', 1700, ': ', 149.10393981933595)\n",
      "('Average loss at step ', 1710, ': ', 149.73848114013671)\n",
      "('Average loss at step ', 1720, ': ', 151.63177947998048)\n",
      "('Average loss at step ', 1730, ': ', 147.45819396972655)\n",
      "('Average loss at step ', 1740, ': ', 149.73365478515626)\n",
      "('Average loss at step ', 1750, ': ', 148.57273101806641)\n",
      "('Average loss at step ', 1760, ': ', 148.03188781738282)\n",
      "('Average loss at step ', 1770, ': ', 148.23778228759767)\n",
      "('Average loss at step ', 1780, ': ', 151.15778961181641)\n",
      "('Average loss at step ', 1790, ': ', 150.03422393798829)\n",
      "('Average loss at step ', 1800, ': ', 147.73382568359375)\n",
      "Nearest to 10673: 10472, 21923, 42463, 15313, 43522, 14312, 41233, 21012,\n",
      "Nearest to 10491: 43491, 12603, 13032, 21592, 15063, 11722, 10652, 11402,\n",
      "Nearest to 10522: 21582, 32262, 42222, 43163, 50062, 15663, 14732, 12142,\n",
      "Nearest to 10062: 21591, 11711, 42263, 12663, 14754, 12401, 11303, 17023,\n",
      "Nearest to 10773: 15332, 11713, 36452, 10635, 12894, 10142, 30922, 13072,\n",
      "Nearest to 10242: 43943, 13011, 12393, 13881, 14276, 43183, 13501, 12183,\n",
      "Nearest to 10106: 42244, 10751, 42351, 60293, 12562, 11522, 22055, 19032,\n",
      "Nearest to 10333: 11133, 30912, 17101, 24383, 42111, 60291, 42091, 12022,\n",
      "Nearest to 10202: 20266, 43021, 23263, 10181, 42403, 30391, 11023, 13823,\n",
      "Nearest to 10661: 19162, 12241, 50563, 14312, 50562, 10413, 12973, 12002,\n",
      "Nearest to 10221: 14753, 11053, 14772, 17272, 14072, 13081, 14212, 11521,\n",
      "Nearest to 10642: 18045, 47584, 19933, 10782, 13583, 12893, 43552, 42463,\n",
      "Nearest to 10331: 43241, 60052, 44295, 14274, 12532, 43512, 14225, 10652,\n",
      "Nearest to 10782: 12023, 43323, 22102, 10642, 44282, 42864, 14631, 12242,\n",
      "Nearest to 10544: 42661, 42503, 11492, 12121, 13316, 44206, 42631, 14732,\n",
      "Nearest to 10531: 13351, 11841, 13233, 50224, 39155, 42331, 43483, 18031,\n",
      "('Average loss at step ', 1810, ': ', 146.00775299072265)\n",
      "('Average loss at step ', 1820, ': ', 146.05378875732421)\n",
      "('Average loss at step ', 1830, ': ', 151.16559143066405)\n",
      "('Average loss at step ', 1840, ': ', 144.38593902587891)\n",
      "('Average loss at step ', 1850, ': ', 148.33374938964843)\n",
      "('Average loss at step ', 1860, ': ', 147.19160156250001)\n",
      "('Average loss at step ', 1870, ': ', 151.35552825927735)\n",
      "('Average loss at step ', 1880, ': ', 148.71238250732421)\n",
      "('Average loss at step ', 1890, ': ', 147.04465942382814)\n",
      "('Average loss at step ', 1900, ': ', 149.69537506103515)\n",
      "('Average loss at step ', 1910, ': ', 149.32475891113282)\n",
      "('Average loss at step ', 1920, ': ', 150.10090026855468)\n",
      "('Average loss at step ', 1930, ': ', 143.97463989257812)\n",
      "('Average loss at step ', 1940, ': ', 147.10411376953124)\n",
      "('Average loss at step ', 1950, ': ', 151.69633636474609)\n",
      "('Average loss at step ', 1960, ': ', 150.51152801513672)\n",
      "('Average loss at step ', 1970, ': ', 150.18948516845703)\n",
      "('Average loss at step ', 1980, ': ', 150.60079956054688)\n",
      "('Average loss at step ', 1990, ': ', 148.91552734375)\n",
      "Softmax method took 18.640193 minutes to run 100\n"
     ]
    }
   ],
   "source": [
    "session=tf.Session(graph=graph)\n",
    "average_loss = 0\n",
    "softmax_start_time = dt.datetime.now()\n",
    "for step in range(num_steps):\n",
    "    session.run(init)\n",
    "    batch_inputs, batch_context = generate_batch(data,batch_size, num_skips, skip_window)\n",
    "    #print batch_inputs,batch_context\n",
    "    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "    #print feed_dict\n",
    "    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "    #_, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "    #print step\n",
    "    if step % 10 == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= 10\n",
    "    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 200 == 0:\n",
    "        sim = similarity.eval(session=session)\n",
    "        for i in range(valid_size):\n",
    "            valid_station = indices_station[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_station\n",
    "            for k in range(top_k):\n",
    "                close_station = indices_station[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_station)\n",
    "            print(log_str)\n",
    "final_embeddings = normalized_embeddings.eval(session=session)\n",
    "softmax_end_time = dt.datetime.now()\n",
    "print(\"Softmax method took {} minutes to run\".format((softmax_end_time-softmax_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
