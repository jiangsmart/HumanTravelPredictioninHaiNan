{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入训练路径\n",
    "train_path = '/home/jiang/data/sorted10000'\n",
    "# 基站路径\n",
    "station_path = '/home/jiang/data/stationRecord'\n",
    "# 输入长度\n",
    "maxlen = 150\n",
    "# 输入中某地（如海口）大于该值，进入训练集\n",
    "threshold = 120\n",
    "# 滚动步长\n",
    "step = 1\n",
    "# 测试长度\n",
    "testlen = 50\n",
    "batch_size=16\n",
    "num_skips=2\n",
    "skip_window=2\n",
    "data_index1=0\n",
    "data_index2=0\n",
    "station_size=200\n",
    "lr=0.01\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "num_steps = 2000\n",
    "\n",
    "station_size = 10000\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(train_path):\n",
    "    file_object = open(train_path)\n",
    "    train_list = file_object.readlines()\n",
    "    train_set=[]\n",
    "    user_record=[]\n",
    "    station_list=[]\n",
    "    station_set=set()\n",
    "    for i,record in enumerate(train_list):\n",
    "        record=record.strip()\n",
    "        user,station=(record.split(\",\")[j] for j in [0,3])\n",
    "        station_set.add(int(station))\n",
    "        if i == len(train_list)-1:\n",
    "            user_record.append(int(station))\n",
    "            train_set.append(user_record)\n",
    "            station_list=sorted(list(station_set))\n",
    "            return train_set,station_list\n",
    "        elif user != train_list[i+1].split(\",\")[0]:\n",
    "            user_record.append(int(station))\n",
    "            train_set.append(user_record)\n",
    "            user_record=[]\n",
    "        else:\n",
    "            user_record.append(int(station))\n",
    "    #return train_set,station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意不要让序列小于3个的序列进入\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index1\n",
    "    global data_index2\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    #print len(data[data_index1])\n",
    "    for i in range(batch_size // num_skips):\n",
    "        #第一次进入循环初始化buffer\n",
    "        if i == 0:\n",
    "            while len(data[data_index1])-data_index2 < span:\n",
    "                data_index2 = 0\n",
    "                data_index1 = (data_index1 + 1)%len(data)                \n",
    "            for _ in range(span-1):\n",
    "                buffer.append(data[data_index1][data_index2])\n",
    "                data_index2 += 1\n",
    "        ###############################\n",
    "        #index更新后判断是否越界\n",
    "        if data_index2 == (len(data[data_index1])):\n",
    "            data_index2 = 0\n",
    "            data_index1 = (data_index1 + 1)%len(data)\n",
    "            while len(data[data_index1])-data_index2 < span:\n",
    "                data_index1 = (data_index1 + 1)%len(data)\n",
    "            for _ in range(span-1):\n",
    "                buffer.append(data[data_index1][data_index2])\n",
    "                data_index2 += 1\n",
    "        #################################\n",
    "        #print data_index1,data_index2 \n",
    "        buffer.append(data[data_index1][data_index2])\n",
    "        #print buffer\n",
    "        data_index2 += 1\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    #data_index2 = (data_index2 + len(data) - span) % len(data)\n",
    "    return batch, context\n",
    "# ########\n",
    "# data_index1=0\n",
    "# data_index2=0\n",
    "# train_data=read_data(train_path)\n",
    "# print train_data\n",
    "# generate_batch(train_data, batch_size, num_skips, skip_window)\n",
    "\n",
    "###################\n",
    "#train_data=read_data(train_path)\n",
    "#generate_batch(train_data, batch_size, num_skips, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_station(station_path):\n",
    "    file_object = open(station_path)\n",
    "    station_list=list()\n",
    "\n",
    "    for line in file_object.readlines():\n",
    "        line = line.strip()\n",
    "        station_list.append(line)\n",
    "\n",
    "    sum_station = len(station_list)# 3000+\n",
    "    #print (station_list)\n",
    "    print(\"sum_station:\", sum_station)\n",
    "    station_indices = dict((c, i) for i, c in enumerate(station_list))\n",
    "    indices_station = dict((i, c) for i, c in enumerate(station_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recount_data(data,station_indices):\n",
    "    new_data=[]\n",
    "    for record in data:\n",
    "        new_record=[]\n",
    "        for station in record:\n",
    "            new_record.append(station_indices[station])\n",
    "        new_data.append(new_record)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_pre(train_path):\n",
    "    train_data,station_data=read_data(train_path)\n",
    "    sum_station = len(station_data)\n",
    "    station_indices = dict((c, i) for i, c in enumerate(station_data))\n",
    "    indices_station = dict((i, c) for i, c in enumerate(station_data))\n",
    "    train_data = recount_data(train_data,station_indices)\n",
    "    return train_data,sum_station,indices_station\n",
    "#generate_batch(train_data, batch_size, num_skips, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183\n"
     ]
    }
   ],
   "source": [
    "data,vocabulary_size,indices_station=data_pre(train_path)\n",
    "print vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    \n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(nce_loss)\n",
    "    \n",
    "#   # Construct the variables for the softmax\n",
    "#     weights = tf.Variable(tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "#                                               stddev=1.0 / math.sqrt(embedding_size)))\n",
    "#     biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "#     hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "\n",
    "#   # convert train_context to a one-hot format\n",
    "#     train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "\n",
    "#     cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "\n",
    "#   # Construct the SGD optimizer using a learning rate of lr.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to 10401: 13583, 10635, 47312, 19091, 19161, 11391, 10421, 42023,\n",
      "Nearest to 10021: 10635, 10642, 40303, 42921, 14383, 12242, 50021, 12263,\n",
      "Nearest to 10331: 14072, 22036, 13583, 19022, 30723, 32961, 42321, 47302,\n",
      "Nearest to 10273: 12541, 50093, 12012, 10653, 10752, 10643, 11301, 12553,\n",
      "Nearest to 10523: 30086, 43711, 50093, 22193, 11832, 14042, 43312, 30251,\n",
      "Nearest to 10732: 10151, 11703, 38762, 43865, 47065, 31071, 15413, 14531,\n",
      "Nearest to 10143: 14041, 25793, 10493, 50511, 10926, 18045, 43973, 40482,\n",
      "Nearest to 10463: 24263, 11022, 19011, 40833, 13023, 10512, 12131, 10933,\n",
      "Nearest to 10161: 22193, 10491, 12901, 10771, 10603, 44351, 43646, 43966,\n",
      "Nearest to 10223: 42151, 11914, 13471, 43171, 50561, 32461, 39113, 17042,\n",
      "Nearest to 10635: 10021, 32861, 43001, 30743, 42503, 10401, 11342, 30153,\n",
      "Nearest to 10181: 42811, 32961, 24581, 12602, 50112, 11721, 43292, 13372,\n",
      "Nearest to 10822: 11481, 11483, 42323, 10242, 43491, 13102, 14213, 47583,\n",
      "Nearest to 10413: 10771, 42652, 10471, 21583, 12183, 21582, 47302, 47182,\n",
      "Nearest to 10411: 11003, 31181, 11483, 17501, 12501, 10643, 11731, 24503,\n",
      "Nearest to 10201: 13763, 23293, 43241, 13731, 44352, 17021, 14812, 42092,\n",
      "('Average loss at step ', 10, ': ', 162.9361541748047)\n",
      "('Average loss at step ', 20, ': ', 146.30632019042969)\n",
      "('Average loss at step ', 30, ': ', 148.09071502685546)\n",
      "('Average loss at step ', 40, ': ', 143.64851074218751)\n",
      "('Average loss at step ', 50, ': ', 150.09071044921876)\n",
      "('Average loss at step ', 60, ': ', 149.41900787353515)\n",
      "('Average loss at step ', 70, ': ', 145.08971099853517)\n",
      "('Average loss at step ', 80, ': ', 150.78272552490233)\n",
      "('Average loss at step ', 90, ': ', 147.89829711914064)\n",
      "('Average loss at step ', 100, ': ', 146.6892074584961)\n",
      "('Average loss at step ', 110, ': ', 146.35072174072266)\n",
      "('Average loss at step ', 120, ': ', 147.85566558837891)\n",
      "('Average loss at step ', 130, ': ', 150.6842010498047)\n",
      "('Average loss at step ', 140, ': ', 154.55330047607421)\n",
      "('Average loss at step ', 150, ': ', 143.65641860961915)\n",
      "('Average loss at step ', 160, ': ', 144.30392608642578)\n",
      "('Average loss at step ', 170, ': ', 149.54431915283203)\n",
      "('Average loss at step ', 180, ': ', 147.32189331054687)\n",
      "('Average loss at step ', 190, ': ', 147.44483795166016)\n",
      "('Average loss at step ', 200, ': ', 153.24822387695312)\n",
      "Nearest to 10401: 43021, 10661, 18181, 43043, 10631, 13512, 10471, 10762,\n",
      "Nearest to 10021: 42433, 24334, 43181, 11272, 43002, 11211, 13913, 13803,\n",
      "Nearest to 10331: 11921, 18101, 32262, 15673, 10152, 21492, 11625, 42403,\n",
      "Nearest to 10273: 10912, 15393, 39113, 21425, 12221, 11783, 24503, 18852,\n",
      "Nearest to 10523: 11272, 10423, 44803, 13134, 23103, 10972, 50523, 14451,\n",
      "Nearest to 10732: 42462, 18566, 42311, 14761, 23455, 15392, 10951, 10973,\n",
      "Nearest to 10143: 12513, 22803, 43341, 32263, 10231, 17391, 43861, 13852,\n",
      "Nearest to 10463: 11492, 13822, 43041, 43731, 10532, 50281, 12242, 60202,\n",
      "Nearest to 10161: 12142, 42203, 47552, 50116, 47262, 43461, 14601, 15402,\n",
      "Nearest to 10223: 21011, 12013, 13113, 13131, 42193, 11533, 44032, 13472,\n",
      "Nearest to 10635: 18881, 30766, 40352, 21483, 42121, 50116, 11702, 50111,\n",
      "Nearest to 10181: 42921, 11613, 13573, 17263, 13511, 32961, 39155, 60631,\n",
      "Nearest to 10822: 44206, 11213, 10311, 14192, 21043, 50021, 11962, 24503,\n",
      "Nearest to 10413: 43761, 43002, 38154, 13133, 11532, 13792, 42312, 50563,\n",
      "Nearest to 10411: 19162, 13523, 19213, 42865, 10901, 42351, 44293, 19265,\n",
      "Nearest to 10201: 43863, 43362, 13123, 60052, 10295, 42773, 31033, 47262,\n",
      "('Average loss at step ', 210, ': ', 148.3583953857422)\n",
      "('Average loss at step ', 220, ': ', 150.23801879882814)\n",
      "('Average loss at step ', 230, ': ', 150.13113708496093)\n",
      "('Average loss at step ', 240, ': ', 149.09155883789063)\n",
      "('Average loss at step ', 250, ': ', 152.05885620117186)\n",
      "('Average loss at step ', 260, ': ', 150.11942749023439)\n",
      "('Average loss at step ', 270, ': ', 151.86207427978516)\n",
      "('Average loss at step ', 280, ': ', 145.17369995117187)\n",
      "('Average loss at step ', 290, ': ', 154.00399169921874)\n",
      "('Average loss at step ', 300, ': ', 149.21375274658203)\n",
      "('Average loss at step ', 310, ': ', 145.59091033935547)\n",
      "('Average loss at step ', 320, ': ', 147.52552032470703)\n",
      "('Average loss at step ', 330, ': ', 150.29940948486328)\n",
      "('Average loss at step ', 340, ': ', 149.42185668945314)\n",
      "('Average loss at step ', 350, ': ', 150.49117736816407)\n",
      "('Average loss at step ', 360, ': ', 147.61733245849609)\n",
      "('Average loss at step ', 370, ': ', 150.5218536376953)\n",
      "('Average loss at step ', 380, ': ', 145.6889762878418)\n",
      "('Average loss at step ', 390, ': ', 150.90406341552733)\n",
      "('Average loss at step ', 400, ': ', 151.41947174072266)\n",
      "Nearest to 10401: 13095, 50061, 43623, 13742, 24332, 42072, 21042, 24287,\n",
      "Nearest to 10021: 42022, 47261, 43251, 43863, 48115, 13852, 15442, 20253,\n",
      "Nearest to 10331: 43623, 13821, 49203, 43163, 10242, 49222, 21482, 32112,\n",
      "Nearest to 10273: 10883, 12502, 42202, 43041, 21142, 13243, 42244, 13541,\n",
      "Nearest to 10523: 17131, 14803, 13582, 13441, 18031, 22452, 11211, 19932,\n",
      "Nearest to 10732: 12792, 42812, 19115, 11963, 10421, 13135, 12131, 10071,\n",
      "Nearest to 10143: 11625, 13072, 44042, 44811, 11713, 42011, 43022, 13223,\n",
      "Nearest to 10463: 12533, 15413, 12661, 14753, 15305, 43253, 11812, 18181,\n",
      "Nearest to 10161: 42624, 22781, 23641, 14753, 13951, 11081, 44611, 22052,\n",
      "Nearest to 10223: 40011, 41383, 47401, 10683, 14522, 14812, 43182, 31262,\n",
      "Nearest to 10635: 15313, 31181, 42793, 13773, 43461, 32313, 13573, 60293,\n",
      "Nearest to 10181: 13852, 42623, 43903, 43221, 13582, 43183, 24012, 11793,\n",
      "Nearest to 10822: 20164, 11921, 15393, 11041, 19071, 39156, 47803, 44033,\n",
      "Nearest to 10413: 30633, 15322, 32111, 42933, 19932, 11541, 30652, 24263,\n",
      "Nearest to 10411: 42204, 11003, 42911, 20164, 23103, 42011, 12663, 11611,\n",
      "Nearest to 10201: 13442, 39183, 43865, 10105, 13762, 43432, 24334, 17323,\n",
      "('Average loss at step ', 410, ': ', 148.31368560791014)\n",
      "('Average loss at step ', 420, ': ', 147.03678741455079)\n",
      "('Average loss at step ', 430, ': ', 151.80076141357421)\n",
      "('Average loss at step ', 440, ': ', 148.33429870605468)\n",
      "('Average loss at step ', 450, ': ', 147.72337493896484)\n",
      "('Average loss at step ', 460, ': ', 147.60361938476564)\n",
      "('Average loss at step ', 470, ': ', 147.47265167236327)\n",
      "('Average loss at step ', 480, ': ', 144.14907989501953)\n",
      "('Average loss at step ', 490, ': ', 146.06419982910157)\n",
      "('Average loss at step ', 500, ': ', 146.87107238769531)\n",
      "('Average loss at step ', 510, ': ', 144.37552947998046)\n",
      "('Average loss at step ', 520, ': ', 147.64045867919921)\n",
      "('Average loss at step ', 530, ': ', 150.21491851806641)\n",
      "('Average loss at step ', 540, ': ', 149.07567138671874)\n",
      "('Average loss at step ', 550, ': ', 153.72589721679688)\n",
      "('Average loss at step ', 560, ': ', 147.74859619140625)\n",
      "('Average loss at step ', 570, ': ', 149.70316009521486)\n",
      "('Average loss at step ', 580, ': ', 150.76426696777344)\n",
      "('Average loss at step ', 590, ': ', 148.78925018310548)\n",
      "('Average loss at step ', 600, ': ', 152.74398193359374)\n",
      "Nearest to 10401: 19493, 39112, 31932, 15192, 41262, 42263, 42033, 11391,\n",
      "Nearest to 10021: 14383, 17272, 50564, 42631, 42422, 43172, 12182, 11142,\n",
      "Nearest to 10331: 10773, 43865, 11492, 43861, 12001, 47572, 42872, 31811,\n",
      "Nearest to 10273: 12312, 12231, 21343, 25781, 12603, 18503, 18252, 10011,\n",
      "Nearest to 10523: 60221, 43182, 13133, 47261, 10643, 39112, 13011, 11742,\n",
      "Nearest to 10732: 39111, 42501, 44451, 13962, 42621, 43513, 13101, 43733,\n",
      "Nearest to 10143: 40834, 18182, 12533, 42513, 10901, 23455, 15681, 39156,\n",
      "Nearest to 10463: 11071, 32262, 47213, 40411, 12661, 18252, 47253, 13373,\n",
      "Nearest to 10161: 17263, 22803, 50061, 32313, 43292, 10913, 42451, 14225,\n",
      "Nearest to 10223: 13032, 10232, 22561, 50111, 14751, 11922, 43291, 30723,\n",
      "Nearest to 10635: 43863, 42591, 20091, 21953, 42163, 22411, 30923, 11531,\n",
      "Nearest to 10181: 11033, 12671, 14522, 11761, 10672, 42513, 10462, 43053,\n",
      "Nearest to 10822: 41351, 43431, 13033, 15402, 40021, 42602, 43496, 10672,\n",
      "Nearest to 10413: 12241, 15571, 44293, 10673, 42091, 44611, 40263, 44351,\n",
      "Nearest to 10411: 12154, 43222, 12433, 60291, 43013, 12641, 11142, 14462,\n",
      "Nearest to 10201: 15051, 13851, 17351, 19272, 11914, 42941, 39112, 38159,\n",
      "('Average loss at step ', 610, ': ', 147.63313598632811)\n",
      "('Average loss at step ', 620, ': ', 149.97801971435547)\n",
      "('Average loss at step ', 630, ': ', 153.38713073730469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step ', 640, ': ', 149.52759857177733)\n",
      "('Average loss at step ', 650, ': ', 150.73258819580079)\n",
      "('Average loss at step ', 660, ': ', 150.63046569824218)\n",
      "('Average loss at step ', 670, ': ', 150.36751861572264)\n",
      "('Average loss at step ', 680, ': ', 151.25957946777345)\n",
      "('Average loss at step ', 690, ': ', 150.13758544921876)\n",
      "('Average loss at step ', 700, ': ', 153.07611846923828)\n",
      "('Average loss at step ', 710, ': ', 148.99404449462889)\n",
      "('Average loss at step ', 720, ': ', 149.57319183349608)\n",
      "('Average loss at step ', 730, ': ', 150.92356262207031)\n",
      "('Average loss at step ', 740, ': ', 152.37877655029297)\n",
      "('Average loss at step ', 750, ': ', 153.60486602783203)\n",
      "('Average loss at step ', 760, ': ', 143.81829223632812)\n",
      "('Average loss at step ', 770, ': ', 149.64235076904296)\n",
      "('Average loss at step ', 780, ': ', 146.93224334716797)\n",
      "('Average loss at step ', 790, ': ', 148.67180023193359)\n",
      "('Average loss at step ', 800, ': ', 149.50785522460939)\n",
      "Nearest to 10401: 14631, 12154, 42811, 14013, 15412, 12661, 40303, 42272,\n",
      "Nearest to 10021: 17661, 50462, 15651, 47022, 47584, 42231, 10106, 42601,\n",
      "Nearest to 10331: 15063, 21042, 21302, 42351, 14733, 42652, 17712, 19272,\n",
      "Nearest to 10273: 20251, 42091, 12516, 13134, 24332, 40483, 43242, 19153,\n",
      "Nearest to 10523: 12843, 22453, 12313, 32263, 50173, 50273, 39111, 11872,\n",
      "Nearest to 10732: 42433, 13762, 13532, 12154, 43042, 11071, 47302, 12503,\n",
      "Nearest to 10143: 13942, 14353, 60243, 10295, 47583, 44044, 10423, 43221,\n",
      "Nearest to 10463: 14741, 17263, 24287, 11762, 11002, 50514, 43983, 11625,\n",
      "Nearest to 10161: 47586, 24287, 10531, 15681, 12482, 12491, 50093, 21421,\n",
      "Nearest to 10223: 10822, 44102, 44302, 42521, 11271, 43122, 42252, 42362,\n",
      "Nearest to 10635: 13582, 21343, 14041, 42222, 42422, 12893, 50243, 11272,\n",
      "Nearest to 10181: 42422, 13942, 43462, 38128, 14903, 41582, 14112, 13351,\n",
      "Nearest to 10822: 11832, 13372, 10223, 15673, 42623, 12553, 10531, 48115,\n",
      "Nearest to 10413: 32861, 43001, 30392, 12541, 12662, 12093, 49203, 10773,\n",
      "Nearest to 10411: 12903, 21042, 10751, 17271, 13751, 12265, 11831, 17391,\n",
      "Nearest to 10201: 42362, 13573, 21582, 31262, 43021, 18181, 19115, 43496,\n",
      "('Average loss at step ', 810, ': ', 149.33292694091796)\n",
      "('Average loss at step ', 820, ': ', 149.77338104248048)\n",
      "('Average loss at step ', 830, ': ', 147.32198486328124)\n",
      "('Average loss at step ', 840, ': ', 152.93186187744141)\n",
      "('Average loss at step ', 850, ': ', 151.34510803222656)\n",
      "('Average loss at step ', 860, ': ', 145.74226684570311)\n",
      "('Average loss at step ', 870, ': ', 148.47491455078125)\n",
      "('Average loss at step ', 880, ': ', 151.81266937255859)\n",
      "('Average loss at step ', 890, ': ', 149.82725830078124)\n",
      "('Average loss at step ', 900, ': ', 148.06898345947266)\n",
      "('Average loss at step ', 910, ': ', 149.06560516357422)\n",
      "('Average loss at step ', 920, ': ', 147.22781372070312)\n",
      "('Average loss at step ', 930, ': ', 149.40052947998046)\n",
      "('Average loss at step ', 940, ': ', 146.9062484741211)\n",
      "('Average loss at step ', 950, ': ', 150.65711669921876)\n",
      "('Average loss at step ', 960, ': ', 148.35054016113281)\n",
      "('Average loss at step ', 970, ': ', 147.45481109619141)\n",
      "('Average loss at step ', 980, ': ', 149.23981170654298)\n",
      "('Average loss at step ', 990, ': ', 147.02933654785156)\n",
      "('Average loss at step ', 1000, ': ', 147.54631805419922)\n",
      "Nearest to 10401: 41591, 19572, 42621, 10747, 13021, 11914, 39113, 11133,\n",
      "Nearest to 10021: 38159, 12481, 10523, 42163, 13591, 40482, 43061, 19221,\n",
      "Nearest to 10331: 17271, 10762, 13573, 42973, 40112, 47531, 60631, 30521,\n",
      "Nearest to 10273: 10841, 13803, 10091, 43312, 11481, 10232, 23072, 11723,\n",
      "Nearest to 10523: 11271, 10021, 30854, 10782, 12903, 30363, 43462, 13933,\n",
      "Nearest to 10732: 42252, 47202, 17712, 11872, 12516, 47212, 13062, 44231,\n",
      "Nearest to 10143: 41625, 30363, 21421, 10463, 43644, 15312, 12461, 19401,\n",
      "Nearest to 10463: 47211, 44233, 12113, 10652, 43741, 10603, 20321, 43403,\n",
      "Nearest to 10161: 13583, 17392, 22282, 13773, 22781, 10432, 18881, 10062,\n",
      "Nearest to 10223: 19282, 50564, 42631, 18432, 17042, 10181, 40112, 12393,\n",
      "Nearest to 10635: 32453, 12121, 17121, 10921, 10841, 15305, 60383, 10181,\n",
      "Nearest to 10181: 13342, 60052, 10223, 22311, 10635, 32262, 21923, 43961,\n",
      "Nearest to 10822: 42302, 14753, 19162, 44351, 23455, 11143, 19172, 11081,\n",
      "Nearest to 10413: 21423, 15491, 11702, 41625, 10747, 42112, 13032, 24262,\n",
      "Nearest to 10411: 43865, 50462, 44301, 11023, 40263, 13503, 14041, 50151,\n",
      "Nearest to 10201: 32961, 19031, 30901, 13273, 44206, 21421, 18182, 44103,\n",
      "('Average loss at step ', 1010, ': ', 147.07599182128905)\n",
      "('Average loss at step ', 1020, ': ', 153.36233673095703)\n",
      "('Average loss at step ', 1030, ': ', 146.56471099853516)\n",
      "('Average loss at step ', 1040, ': ', 150.39614562988282)\n",
      "('Average loss at step ', 1050, ': ', 153.16463012695311)\n",
      "('Average loss at step ', 1060, ': ', 148.52944488525389)\n",
      "('Average loss at step ', 1070, ': ', 149.8289794921875)\n",
      "('Average loss at step ', 1080, ': ', 148.31646728515625)\n",
      "('Average loss at step ', 1090, ': ', 147.14403381347657)\n",
      "('Average loss at step ', 1100, ': ', 148.42240600585939)\n",
      "('Average loss at step ', 1110, ': ', 142.66092453002929)\n",
      "('Average loss at step ', 1120, ': ', 152.38946380615235)\n",
      "('Average loss at step ', 1130, ': ', 148.28763732910156)\n",
      "('Average loss at step ', 1140, ': ', 149.86074066162109)\n",
      "('Average loss at step ', 1150, ': ', 152.27463378906251)\n",
      "('Average loss at step ', 1160, ': ', 148.33072509765626)\n",
      "('Average loss at step ', 1170, ': ', 152.43070068359376)\n",
      "('Average loss at step ', 1180, ': ', 156.07810211181641)\n",
      "('Average loss at step ', 1190, ': ', 155.65083007812501)\n",
      "('Average loss at step ', 1200, ': ', 149.56657562255859)\n",
      "Nearest to 10401: 13731, 42493, 42433, 32034, 19114, 10091, 43511, 40077,\n",
      "Nearest to 10021: 13072, 13561, 16235, 14581, 13501, 21953, 24332, 42501,\n",
      "Nearest to 10331: 32861, 11962, 12516, 24581, 10011, 44172, 10432, 15322,\n",
      "Nearest to 10273: 60052, 20322, 12154, 50561, 10232, 14011, 42662, 11761,\n",
      "Nearest to 10523: 42191, 42193, 15641, 42831, 12211, 42111, 43163, 13032,\n",
      "Nearest to 10732: 50281, 12483, 43002, 21343, 12142, 10293, 42624, 43495,\n",
      "Nearest to 10143: 11081, 43965, 22781, 60631, 44295, 19183, 12514, 14312,\n",
      "Nearest to 10463: 24211, 14013, 20164, 23102, 11041, 21582, 43103, 43262,\n",
      "Nearest to 10161: 15614, 30292, 31462, 10513, 12091, 42801, 43403, 42451,\n",
      "Nearest to 10223: 18432, 48311, 40077, 11713, 12602, 11142, 10092, 11522,\n",
      "Nearest to 10635: 12541, 11262, 47343, 17351, 44351, 15431, 42204, 31352,\n",
      "Nearest to 10181: 38128, 42072, 31811, 12241, 19261, 14081, 43041, 14692,\n",
      "Nearest to 10822: 17132, 19712, 12081, 50121, 12821, 25551, 12721, 42661,\n",
      "Nearest to 10413: 42081, 12822, 50521, 10782, 18251, 18822, 22101, 11813,\n",
      "Nearest to 10411: 43262, 18503, 12553, 13822, 12641, 44551, 13062, 19093,\n",
      "Nearest to 10201: 14112, 43903, 43473, 10423, 11612, 43644, 20253, 43833,\n",
      "('Average loss at step ', 1210, ': ', 143.56928787231445)\n",
      "('Average loss at step ', 1220, ': ', 145.61509094238281)\n",
      "('Average loss at step ', 1230, ': ', 150.40186462402343)\n",
      "('Average loss at step ', 1240, ': ', 150.54964904785157)\n",
      "('Average loss at step ', 1250, ': ', 150.98636169433593)\n",
      "('Average loss at step ', 1260, ': ', 147.16172790527344)\n",
      "('Average loss at step ', 1270, ': ', 146.08015136718751)\n",
      "('Average loss at step ', 1280, ': ', 145.98885955810547)\n",
      "('Average loss at step ', 1290, ': ', 149.14494628906249)\n",
      "('Average loss at step ', 1300, ': ', 149.99772186279296)\n",
      "('Average loss at step ', 1310, ': ', 147.3040786743164)\n",
      "('Average loss at step ', 1320, ': ', 146.18254241943359)\n",
      "('Average loss at step ', 1330, ': ', 149.58775024414064)\n",
      "('Average loss at step ', 1340, ': ', 144.33710327148438)\n",
      "('Average loss at step ', 1350, ': ', 149.5820327758789)\n",
      "('Average loss at step ', 1360, ': ', 148.13661193847656)\n",
      "('Average loss at step ', 1370, ': ', 148.58663024902344)\n",
      "('Average loss at step ', 1380, ': ', 150.36252441406251)\n",
      "('Average loss at step ', 1390, ': ', 147.70439910888672)\n",
      "('Average loss at step ', 1400, ': ', 150.98591613769531)\n",
      "Nearest to 10401: 10106, 21482, 10643, 43831, 30102, 14203, 18851, 13511,\n",
      "Nearest to 10021: 13731, 11711, 17263, 60373, 16235, 50116, 14456, 11391,\n",
      "Nearest to 10331: 43242, 10493, 50224, 42244, 14272, 44044, 22106, 21142,\n",
      "Nearest to 10273: 14741, 50561, 17391, 44204, 47052, 12663, 21424, 22453,\n",
      "Nearest to 10523: 22453, 47312, 18853, 42462, 42244, 10301, 22731, 10271,\n",
      "Nearest to 10732: 14274, 43341, 13762, 10951, 12893, 43431, 11914, 48115,\n",
      "Nearest to 10143: 11821, 11393, 11752, 14722, 43252, 11003, 42203, 42191,\n",
      "Nearest to 10463: 47572, 13742, 43163, 44091, 11702, 42311, 10663, 11083,\n",
      "Nearest to 10161: 44203, 42582, 12113, 43732, 43171, 13962, 43181, 43733,\n",
      "Nearest to 10223: 10522, 43741, 30363, 19021, 39111, 18132, 30766, 14276,\n",
      "Nearest to 10635: 11733, 47586, 17022, 15305, 47582, 12671, 12483, 42621,\n",
      "Nearest to 10181: 11231, 13773, 12113, 15062, 39102, 50113, 15322, 19031,\n",
      "Nearest to 10822: 19032, 42292, 14602, 21582, 14072, 10911, 10913, 21513,\n",
      "Nearest to 10413: 13953, 43833, 10643, 47571, 24056, 24263, 42623, 19161,\n",
      "Nearest to 10411: 20323, 11261, 14813, 18566, 12662, 50061, 13231, 44206,\n",
      "Nearest to 10201: 43361, 10772, 24012, 43426, 43171, 10782, 47262, 43943,\n",
      "('Average loss at step ', 1410, ': ', 148.6644256591797)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average loss at step ', 1420, ': ', 148.86983795166014)\n",
      "('Average loss at step ', 1430, ': ', 148.41142578124999)\n",
      "('Average loss at step ', 1440, ': ', 151.69361267089843)\n",
      "('Average loss at step ', 1450, ': ', 151.23750305175781)\n",
      "('Average loss at step ', 1460, ': ', 147.98248443603515)\n",
      "('Average loss at step ', 1470, ': ', 144.46285095214844)\n",
      "('Average loss at step ', 1480, ': ', 151.08172454833985)\n",
      "('Average loss at step ', 1490, ': ', 149.90055236816406)\n",
      "('Average loss at step ', 1500, ': ', 149.60350189208984)\n",
      "('Average loss at step ', 1510, ': ', 150.30287475585936)\n",
      "('Average loss at step ', 1520, ': ', 144.76885375976562)\n",
      "('Average loss at step ', 1530, ': ', 144.38936767578124)\n",
      "('Average loss at step ', 1540, ': ', 143.91622619628907)\n",
      "('Average loss at step ', 1550, ': ', 143.44237823486327)\n",
      "('Average loss at step ', 1560, ': ', 148.70382690429688)\n",
      "('Average loss at step ', 1570, ': ', 149.29779052734375)\n",
      "('Average loss at step ', 1580, ': ', 154.79888763427735)\n",
      "('Average loss at step ', 1590, ': ', 148.61458435058594)\n",
      "('Average loss at step ', 1600, ': ', 151.06879730224608)\n",
      "Nearest to 10401: 11872, 41582, 42491, 17221, 42244, 22311, 11061, 11331,\n",
      "Nearest to 10021: 20053, 13211, 21492, 23455, 41624, 50243, 42641, 11041,\n",
      "Nearest to 10331: 18433, 19153, 43022, 13316, 10762, 10751, 12843, 11143,\n",
      "Nearest to 10273: 47343, 42151, 13441, 21302, 30854, 11211, 11303, 12391,\n",
      "Nearest to 10523: 11271, 17272, 17132, 12183, 12896, 43283, 43833, 32501,\n",
      "Nearest to 10732: 42111, 48151, 32363, 14756, 14223, 10462, 43363, 44206,\n",
      "Nearest to 10143: 42313, 25781, 13122, 50273, 47053, 32462, 11042, 42421,\n",
      "Nearest to 10463: 12602, 42121, 10671, 42865, 44301, 43183, 30766, 50093,\n",
      "Nearest to 10161: 44803, 11083, 42651, 44172, 10771, 14303, 13221, 10092,\n",
      "Nearest to 10223: 23455, 17271, 19282, 12641, 12393, 43552, 50562, 17501,\n",
      "Nearest to 10635: 12893, 12922, 18251, 42401, 21302, 11713, 31071, 17121,\n",
      "Nearest to 10181: 11331, 11916, 19071, 13842, 17221, 42222, 42092, 42661,\n",
      "Nearest to 10822: 11711, 31691, 12242, 36833, 42813, 10883, 60373, 17221,\n",
      "Nearest to 10413: 50112, 10773, 47262, 15322, 14353, 10202, 13762, 43181,\n",
      "Nearest to 10411: 10202, 42222, 10771, 12516, 13881, 13371, 10522, 11053,\n",
      "Nearest to 10201: 14041, 14071, 15391, 11461, 42951, 18256, 32396, 11303,\n",
      "('Average loss at step ', 1610, ': ', 147.63466796874999)\n",
      "('Average loss at step ', 1620, ': ', 147.64411010742188)\n",
      "('Average loss at step ', 1630, ': ', 146.62422637939454)\n",
      "('Average loss at step ', 1640, ': ', 141.62645568847657)\n",
      "('Average loss at step ', 1650, ': ', 153.71624755859375)\n",
      "('Average loss at step ', 1660, ': ', 152.26859283447266)\n",
      "('Average loss at step ', 1670, ': ', 144.95747375488281)\n",
      "('Average loss at step ', 1680, ': ', 149.57828063964843)\n",
      "('Average loss at step ', 1690, ': ', 148.40971527099609)\n",
      "('Average loss at step ', 1700, ': ', 154.95905151367188)\n",
      "('Average loss at step ', 1710, ': ', 151.84196777343749)\n",
      "('Average loss at step ', 1720, ': ', 148.04753723144532)\n",
      "('Average loss at step ', 1730, ': ', 147.48551330566406)\n",
      "('Average loss at step ', 1740, ': ', 147.37766265869141)\n",
      "('Average loss at step ', 1750, ': ', 148.11175231933595)\n",
      "('Average loss at step ', 1760, ': ', 146.31056365966796)\n",
      "('Average loss at step ', 1770, ': ', 149.45319976806641)\n",
      "('Average loss at step ', 1780, ': ', 152.17899780273439)\n",
      "('Average loss at step ', 1790, ': ', 149.47582550048827)\n",
      "('Average loss at step ', 1800, ': ', 149.38818359375)\n",
      "Nearest to 10401: 22561, 42202, 48132, 19572, 19183, 11003, 11733, 15431,\n",
      "Nearest to 10021: 15091, 47571, 50092, 12211, 10332, 42421, 10544, 11271,\n",
      "Nearest to 10331: 19172, 19572, 42523, 14012, 11723, 43461, 30912, 13841,\n",
      "Nearest to 10273: 14012, 13583, 32462, 15322, 11491, 10152, 10232, 42971,\n",
      "Nearest to 10523: 10673, 10251, 50562, 13343, 43762, 13881, 43646, 11383,\n",
      "Nearest to 10732: 12652, 11941, 42353, 24332, 40483, 40011, 10701, 39111,\n",
      "Nearest to 10143: 15392, 13223, 12183, 19801, 11292, 22661, 15062, 14452,\n",
      "Nearest to 10463: 10663, 21301, 42503, 14722, 12531, 30743, 42642, 12061,\n",
      "Nearest to 10161: 11001, 47586, 42203, 11531, 43323, 18182, 47253, 12391,\n",
      "Nearest to 10223: 13933, 50093, 10951, 17023, 15681, 10202, 13123, 12091,\n",
      "Nearest to 10635: 10683, 47531, 14212, 12081, 10771, 22452, 44293, 47261,\n",
      "Nearest to 10181: 10091, 14456, 12023, 12561, 17173, 44211, 13841, 12263,\n",
      "Nearest to 10822: 43403, 47201, 22106, 32981, 10081, 42163, 42452, 11081,\n",
      "Nearest to 10413: 15403, 42362, 42582, 24334, 50523, 42642, 14833, 10631,\n",
      "Nearest to 10411: 12083, 14522, 42431, 13571, 10471, 10104, 12391, 30671,\n",
      "Nearest to 10201: 10972, 10311, 13033, 50442, 44233, 22873, 60473, 14812,\n",
      "('Average loss at step ', 1810, ': ', 149.28589782714843)\n",
      "('Average loss at step ', 1820, ': ', 153.25215301513671)\n",
      "('Average loss at step ', 1830, ': ', 141.71595916748046)\n",
      "('Average loss at step ', 1840, ': ', 156.02352294921874)\n",
      "('Average loss at step ', 1850, ': ', 149.04743347167968)\n",
      "('Average loss at step ', 1860, ': ', 151.82827606201172)\n",
      "('Average loss at step ', 1870, ': ', 148.49460449218751)\n",
      "('Average loss at step ', 1880, ': ', 143.3673095703125)\n",
      "('Average loss at step ', 1890, ': ', 152.04011535644531)\n",
      "('Average loss at step ', 1900, ': ', 146.99825744628907)\n",
      "('Average loss at step ', 1910, ': ', 146.36186065673829)\n",
      "('Average loss at step ', 1920, ': ', 145.88884277343749)\n",
      "('Average loss at step ', 1930, ': ', 150.40272064208983)\n",
      "('Average loss at step ', 1940, ': ', 148.58624725341798)\n",
      "('Average loss at step ', 1950, ': ', 149.96882171630858)\n",
      "('Average loss at step ', 1960, ': ', 147.79453887939454)\n",
      "('Average loss at step ', 1970, ': ', 145.73282623291016)\n",
      "('Average loss at step ', 1980, ': ', 146.79706420898438)\n",
      "('Average loss at step ', 1990, ': ', 147.58672637939452)\n",
      "Softmax method took 15.484816 minutes to run 100       iterations\n"
     ]
    }
   ],
   "source": [
    "session=tf.Session(graph=graph)\n",
    "average_loss = 0\n",
    "softmax_start_time = dt.datetime.now()\n",
    "for step in range(num_steps):\n",
    "    session.run(init)\n",
    "    batch_inputs, batch_context = generate_batch(data,batch_size, num_skips, skip_window)\n",
    "    #print batch_inputs,batch_context\n",
    "    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "    #print feed_dict\n",
    "    #session.run(embedding,train_context)\n",
    "    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "    #_, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "    #print step\n",
    "    if step % 10 == 0:\n",
    "        if step > 0:\n",
    "            average_loss /= 10\n",
    "    # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 200 == 0:\n",
    "        sim = similarity.eval(session=session)\n",
    "        for i in range(valid_size):\n",
    "            valid_word = indices_station[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = indices_station[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "final_embeddings = normalized_embeddings.eval(session=session)\n",
    "softmax_end_time = dt.datetime.now()\n",
    "print(\"Softmax method took {} minutes to run 100 \\\n",
    "      iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05332918 -0.02141058  0.03987438 ...,  0.02171163  0.00181332\n",
      "   0.03082538]\n",
      " [ 0.01776588 -0.06811112 -0.01671704 ..., -0.09330609  0.05481327\n",
      "   0.08938831]\n",
      " [-0.02200251  0.00138551 -0.09162377 ...,  0.01695991  0.02243578\n",
      "  -0.05716919]\n",
      " ..., \n",
      " [-0.04669112  0.06974588  0.03538474 ..., -0.03105536 -0.09428776\n",
      "  -0.06094928]\n",
      " [ 0.08359805 -0.08983115  0.09245062 ..., -0.02476301 -0.04803494\n",
      "  -0.03809367]\n",
      " [-0.00273002  0.0944417  -0.06332336 ...,  0.10173488 -0.0564303\n",
      "  -0.06549655]]\n"
     ]
    }
   ],
   "source": [
    "print final_embeddings\n",
    "\n",
    "# def run(graph, num_steps):\n",
    "#     with tf.Session(graph=graph) as session:\n",
    "#       # We must initialize all variables before we use them.\n",
    "#         init.run()\n",
    "#         print('Initialized')\n",
    "\n",
    "#         average_loss = 0\n",
    "#         for step in range(num_steps):\n",
    "#             batch_inputs, batch_context = generate_batch(data,batch_size, num_skips, skip_window)\n",
    "#             print batch_inputs,batch_context\n",
    "#             feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "#             # We perform one update step by evaluating the optimizer op (including it\n",
    "#             # in the list of returned values for session.run()\n",
    "#             _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "#             average_loss += loss_val\n",
    "\n",
    "#             if step % 2000 == 0:\n",
    "#                 if step > 0:\n",
    "#                     average_loss /= 2000\n",
    "#               # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "#                     print('Average loss at step ', step, ': ', average_loss)\n",
    "#                     average_loss = 0\n",
    "\n",
    "#             # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#             if step % 10000 == 0:\n",
    "#                 sim = similarity.eval()\n",
    "#                 for i in range(valid_size):\n",
    "#                     valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#                     top_k = 8  # number of nearest neighbors\n",
    "#                     nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "#                     log_str = 'Nearest to %s:' % valid_word\n",
    "#                     for k in range(top_k):\n",
    "#                         close_word = reverse_dictionary[nearest[k]]\n",
    "#                         log_str = '%s %s,' % (log_str, close_word)\n",
    "#                     print(log_str)\n",
    "#         final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "# num_steps = 100\n",
    "# softmax_start_time = dt.datetime.now()\n",
    "# run(graph, num_steps=num_steps)\n",
    "# softmax_end_time = dt.datetime.now()\n",
    "# print(\"Softmax method took {} minutes to run 100 \\\n",
    "#       iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))\n",
    "\n",
    "# with graph.as_default():\n",
    "\n",
    "#     # Construct the variables for the NCE loss\n",
    "#     nce_weights = tf.Variable(\n",
    "#         tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "#                             stddev=1.0 / math.sqrt(embedding_size)))\n",
    "#     nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "#     nce_loss = tf.reduce_mean(\n",
    "#         tf.nn.nce_loss(weights=nce_weights,\n",
    "#                        biases=nce_biases,\n",
    "#                        labels=train_context,\n",
    "#                        inputs=embed,\n",
    "#                        num_sampled=num_sampled,\n",
    "#                        num_classes=vocabulary_size))\n",
    "\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "#     # Add variable initializer.\n",
    "#     init = tf.global_variables_initializer()\n",
    "\n",
    "# num_steps = 50000\n",
    "# nce_start_time = dt.datetime.now()\n",
    "# run(graph, num_steps)\n",
    "# nce_end_time = dt.datetime.now()\n",
    "# print(\"NCE method took {} minutes to run 100 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
